{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678cf425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from airflow.hooks.base_hook import BaseHook\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b36f837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .config('spark.driver.extraClassPath', '/home/user/shared/postgresql-42.3.1.jar')\\\n",
    "    .master('local')\\\n",
    "    .appName('homework_lesson_16')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc0b4554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://0.0.0.0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>homework_lesson_16</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f261aa370d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b787558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-11-15 19:01:31,670] {base_hook.py:89} INFO - Using connection to: id: DWH. Host: 192.168.1.249, Port: 5433, Schema: postgres, Login: gpuser, Password: XXXXXXXX, extra: None\n"
     ]
    }
   ],
   "source": [
    "gp_conn = BaseHook.get_connection('DWH')\n",
    "gp_url = f\"jdbc:postgresql://{gp_conn.host}:{gp_conn.port}/{gp_conn.schema}\"\n",
    "gp_creds = {\"user\":gp_conn.login, \"password\":gp_conn.password}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b5bc213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jdbc:postgresql://192.168.1.249:5433/postgres\n"
     ]
    }
   ],
   "source": [
    "print(gp_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1b6e68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user': 'gpuser', 'password': 'secret'}\n"
     ]
    }
   ],
   "source": [
    "print(gp_creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278c58d",
   "metadata": {},
   "source": [
    "# Dimension Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44552026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_silver_clients = spark.read.parquet(\"/silver/dshop/clients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f3544b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_clients = df_silver_clients\\\n",
    "    .select('client_id', F.col('fullname').alias('client_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "702dc85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_dim_clients.write.jdbc(gp_url, table = 'dim_clients', properties = gp_creds, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02535533",
   "metadata": {},
   "source": [
    "# Dimension Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff96d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver_aisles = spark.read.parquet(\"/silver/dshop/aisles\")\n",
    "df_silver_departments = spark.read.parquet(\"/silver/dshop/departments\")\n",
    "df_silver_products = spark.read.parquet(\"/silver/dshop/products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afedd9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_products = df_silver_products\\\n",
    "    .join(df_silver_aisles, 'aisle_id', 'left')\\\n",
    "    .join(df_silver_departments, 'department_id', 'left')\\\n",
    "    .select('product_id', 'product_name', 'aisle', 'department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "620574b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_dim_products.write.jdbc(gp_url, table = 'dim_products', properties = gp_creds, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d932d25",
   "metadata": {},
   "source": [
    "# Dimension Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ffe9101",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_period = spark.createDataFrame([\"2000-01-01\"], \"string\").toDF(\"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc2d807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_period = df_period.withColumn(\"stop\", F.current_date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffbba807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 8:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start, stop = df_period.select([F.col(c).cast(\"timestamp\").cast(\"long\") for c in (\"start\", \"stop\")])\\\n",
    "    .first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3570a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_date = spark.range(start, stop, 24*60*60)\\\n",
    "    .select(F.col(\"id\").cast(\"timestamp\").cast(\"date\").alias(\"date\"))\\\n",
    "    .withColumn(\"year\", F.year(\"date\"))\\\n",
    "    .withColumn(\"month\", F.month(\"date\"))\\\n",
    "    .withColumn(\"day\", F.dayofmonth(\"date\"))\\\n",
    "    .withColumn(\"day_of_week\", F.dayofweek(\"date\"))\\\n",
    "    .withColumn(\"day_of_year\", F.dayofyear(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23f91ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_dim_date.write.jdbc(gp_url, table = 'dim_date', properties = gp_creds, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f4b5ff",
   "metadata": {},
   "source": [
    "# Fact Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e45c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver_orders = spark.read.parquet(\"/silver/dshop/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74e5b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_orders = df_silver_orders\\\n",
    "    .select('order_id', 'product_id', 'client_id', 'order_date', 'quantity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba279d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fact_orders.write.jdbc(gp_url, table = 'fact_orders', properties = gp_creds, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ff7e09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read actual clients from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25f68288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_clients = spark.read.jdbc(gp_url, table = 'dim_clients', properties = gp_creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b28061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_dim_clients = df_fact_orders\\\n",
    "    .join(df_dim_clients, 'client_id', 'left_anti')\\\n",
    "    .groupby('client_id')\\\n",
    "    .count()\\\n",
    "    .withColumn('client_name', F.lit('Unknown'))\\\n",
    "    .select('client_id', 'client_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5083e298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_missing_dim_clients.write.jdbc(gp_url, table = 'dim_clients', properties = gp_creds, mode='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67448bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read actual products from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbf57d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_products = spark.read.jdbc(gp_url, table = 'dim_products', properties = gp_creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0eae5a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_dim_products = df_fact_orders\\\n",
    "    .join(df_dim_products, 'product_id', 'left_anti')\\\n",
    "    .groupby('product_id')\\\n",
    "    .count()\\\n",
    "    .withColumn('product_name', F.lit('Unknown'))\\\n",
    "    .withColumn('aisle', F.lit('Unknown'))\\\n",
    "    .withColumn('department', F.lit('Unknown'))\\\n",
    "    .select('product_id', 'product_name', 'aisle', 'department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78f9f657",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_missing_dim_products.write.jdbc(gp_url, table = 'dim_products', properties = gp_creds, mode='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9a0ffb",
   "metadata": {},
   "source": [
    "\n",
    "# Fact out_of_stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53e6c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver_out_of_stock_app = spark.read.parquet(\"/silver/out_of_stock_app/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c44dd74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_out_of_stock = df_silver_out_of_stock_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0694caea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_fact_out_of_stock.write.jdbc(gp_url, table = 'fact_out_of_stock', properties = gp_creds, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4b88b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read actual products from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f1664f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_products = spark.read.jdbc(gp_url, table = 'dim_products', properties = gp_creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4dab4de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_dim_products = df_fact_out_of_stock\\\n",
    "    .join(df_dim_products, 'product_id', 'left_anti')\\\n",
    "    .groupby('product_id')\\\n",
    "    .count()\\\n",
    "    .withColumn('product_name', F.lit('Unknown'))\\\n",
    "    .withColumn('aisle', F.lit('Unknown'))\\\n",
    "    .withColumn('department', F.lit('Unknown'))\\\n",
    "    .select('product_id', 'product_name', 'aisle', 'department')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53743157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_missing_dim_products.write.jdbc(gp_url, table = 'dim_products', properties = gp_creds, mode='append')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
